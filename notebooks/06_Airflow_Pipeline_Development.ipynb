{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úàÔ∏è Notebook 6: Airflow Pipeline Development\n",
    "\n",
    "**Author:** Amey Talkatkar | **Course:** MLOps with Agentic AI\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Understand Airflow DAG structure\n",
    "- Create tasks with different operators\n",
    "- Define task dependencies\n",
    "- Use XCom for inter-task communication\n",
    "- Build end-to-end ML training pipeline\n",
    "- Test and debug DAGs\n",
    "\n",
    "## üî• The Problem\n",
    "Current ML workflow:\n",
    "1. Sarah manually runs data pull script at 2 AM\n",
    "2. If it fails, she debugs for hours\n",
    "3. Then runs training script\n",
    "4. If training fails, starts over\n",
    "5. Manually deploys if successful\n",
    "6. Exhausted after 3 months\n",
    "\n",
    "**Airflow Solution:** Automate everything! Run at 2 AM automatically, retry on failures, Sarah sleeps peacefully üò¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Important Note\n",
    "\n",
    "This notebook explains Airflow concepts and shows code examples. \n",
    "\n",
    "**To actually run:** Copy code to `~/airflow/dags/` and trigger via Airflow UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Airflow Basics\n",
    "\n",
    "### What is a DAG?\n",
    "\n",
    "**DAG = Directed Acyclic Graph**\n",
    "\n",
    "```\n",
    "     [Start]\n",
    "        |\n",
    "        v\n",
    "   [Pull Data]\n",
    "        |\n",
    "        v\n",
    "  [Validate Data] --> If invalid, STOP\n",
    "        |\n",
    "        v\n",
    "  [Feature Engineering]\n",
    "        |\n",
    "    +---+---+\n",
    "    |   |   |\n",
    "    v   v   v\n",
    "  [LR][RF][XGB]  <-- Parallel execution\n",
    "    |   |   |\n",
    "    +---+---+\n",
    "        |\n",
    "        v\n",
    "  [Compare Models]\n",
    "        |\n",
    "        v\n",
    "  [Deploy Best]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Simple DAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code shows DAG structure (don't run in notebook)\n",
    "example_code = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Default arguments for all tasks\n",
    "default_args = {\n",
    "    'owner': 'amey',\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define DAG\n",
    "with DAG(\n",
    "    'simple_ml_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Simple ML pipeline',\n",
    "    schedule='@daily',  # Run once per day\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ml', 'training'],\n",
    ") as dag:\n",
    "    \n",
    "    def load_data():\n",
    "        print(\"Loading data...\")\n",
    "        return \"data_loaded\"\n",
    "    \n",
    "    def train_model():\n",
    "        print(\"Training model...\")\n",
    "        return \"model_trained\"\n",
    "    \n",
    "    def deploy_model():\n",
    "        print(\"Deploying model...\")\n",
    "        return \"deployed\"\n",
    "    \n",
    "    # Create tasks\n",
    "    task_load = PythonOperator(\n",
    "        task_id='load_data',\n",
    "        python_callable=load_data,\n",
    "    )\n",
    "    \n",
    "    task_train = PythonOperator(\n",
    "        task_id='train_model',\n",
    "        python_callable=train_model,\n",
    "    )\n",
    "    \n",
    "    task_deploy = PythonOperator(\n",
    "        task_id='deploy_model',\n",
    "        python_callable=deploy_model,\n",
    "    )\n",
    "    \n",
    "    # Define dependencies\n",
    "    task_load >> task_train >> task_deploy\n",
    "'''\n",
    "\n",
    "print(\"Simple DAG Example:\")\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Task Dependencies\n",
    "\n",
    "### Linear (Sequential)\n",
    "```python\n",
    "task_a >> task_b >> task_c\n",
    "# Runs: A ‚Üí B ‚Üí C\n",
    "```\n",
    "\n",
    "### Parallel\n",
    "```python\n",
    "task_a >> [task_b, task_c, task_d] >> task_e\n",
    "# Runs: A ‚Üí (B, C, D in parallel) ‚Üí E\n",
    "```\n",
    "\n",
    "### Branching\n",
    "```python\n",
    "from airflow.operators.python import BranchOperator\n",
    "\n",
    "def choose_branch(**context):\n",
    "    if condition:\n",
    "        return 'deploy_task'\n",
    "    else:\n",
    "        return 'skip_task'\n",
    "\n",
    "branch = BranchOperator(\n",
    "    task_id='branch',\n",
    "    python_callable=choose_branch,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: XCom (Cross-Communication)\n",
    "\n",
    "Pass data between tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcom_example = '''\n",
    "# Task 1: Push data to XCom\n",
    "def task_push(**context):\n",
    "    rmse = 1234.56\n",
    "    context['ti'].xcom_push(key='model_rmse', value=rmse)\n",
    "    return \"rmse_calculated\"\n",
    "\n",
    "# Task 2: Pull data from XCom\n",
    "def task_pull(**context):\n",
    "    rmse = context['ti'].xcom_pull(\n",
    "        task_ids='calculate_rmse',\n",
    "        key='model_rmse'\n",
    "    )\n",
    "    print(f\"Model RMSE: {rmse}\")\n",
    "    \n",
    "    if rmse < 1500:\n",
    "        return \"deploy\"\n",
    "    else:\n",
    "        return \"reject\"\n",
    "'''\n",
    "\n",
    "print(\"XCom Example:\")\n",
    "print(xcom_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Complete ML Training Pipeline\n",
    "\n",
    "Full production-ready DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'amey',\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'email_on_failure': False,\n",
    "}\n",
    "\n",
    "def dvc_pull_data(**context):\n",
    "    \"\"\"Pull latest data from DVC.\"\"\"\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        ['dvc', 'pull', 'data/raw/sales_data.csv.dvc'],\n",
    "        capture_output=True,\n",
    "        cwd='/path/to/project'\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        raise Exception(f\"DVC pull failed: {result.stderr}\")\n",
    "    return \"data_pulled\"\n",
    "\n",
    "def validate_data(**context):\n",
    "    \"\"\"Validate data quality.\"\"\"\n",
    "    df = pd.read_csv('data/raw/sales_data.csv')\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum().sum()\n",
    "    if missing > 0:\n",
    "        raise ValueError(f\"Data has {missing} missing values!\")\n",
    "    \n",
    "    # Check minimum rows\n",
    "    if len(df) < 1000:\n",
    "        raise ValueError(f\"Not enough data: {len(df)} rows\")\n",
    "    \n",
    "    context['ti'].xcom_push(key='num_rows', value=len(df))\n",
    "    return \"data_valid\"\n",
    "\n",
    "def train_model(**context):\n",
    "    \"\"\"Train model with MLflow tracking.\"\"\"\n",
    "    # Load data\n",
    "    X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "    y_train = pd.read_csv('data/processed/y_train.csv').squeeze()\n",
    "    X_test = pd.read_csv('data/processed/X_test.csv')\n",
    "    y_test = pd.read_csv('data/processed/y_test.csv').squeeze()\n",
    "    \n",
    "    # Set MLflow\n",
    "    mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "    mlflow.set_experiment('airflow_pipeline')\n",
    "    \n",
    "    with mlflow.start_run(run_name='random_forest_airflow'):\n",
    "        # Train\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(model.get_params())\n",
    "        mlflow.log_metric('rmse', rmse)\n",
    "        mlflow.sklearn.log_model(model, 'model')\n",
    "        \n",
    "        # Push metrics to XCom\n",
    "        context['ti'].xcom_push(key='rmse', value=float(rmse))\n",
    "        context['ti'].xcom_push(key='run_id', value=mlflow.active_run().info.run_id)\n",
    "        \n",
    "    return \"model_trained\"\n",
    "\n",
    "def evaluate_model(**context):\n",
    "    \"\"\"Decide if model is good enough.\"\"\"\n",
    "    ti = context['ti']\n",
    "    rmse = ti.xcom_pull(task_ids='train_model', key='rmse')\n",
    "    \n",
    "    threshold = 2000  # Business requirement\n",
    "    \n",
    "    if rmse < threshold:\n",
    "        print(f\"‚úÖ Model approved: RMSE {rmse:.2f} < {threshold}\")\n",
    "        return 'register_model'\n",
    "    else:\n",
    "        print(f\"‚ùå Model rejected: RMSE {rmse:.2f} >= {threshold}\")\n",
    "        return 'skip_registration'\n",
    "\n",
    "def register_model(**context):\n",
    "    \"\"\"Register model to MLflow registry.\"\"\"\n",
    "    ti = context['ti']\n",
    "    run_id = ti.xcom_pull(task_ids='train_model', key='run_id')\n",
    "    \n",
    "    mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "    \n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    mv = mlflow.register_model(model_uri, \"sales_forecasting_model\")\n",
    "    \n",
    "    # Transition to Staging\n",
    "    client = mlflow.MlflowClient()\n",
    "    client.transition_model_version_stage(\n",
    "        name=\"sales_forecasting_model\",\n",
    "        version=mv.version,\n",
    "        stage=\"Staging\"\n",
    "    )\n",
    "    \n",
    "    return \"model_registered\"\n",
    "\n",
    "# Define DAG\n",
    "with DAG(\n",
    "    'ml_training_pipeline_complete',\n",
    "    default_args=default_args,\n",
    "    description='Complete ML training pipeline',\n",
    "    schedule='@daily',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ml', 'production'],\n",
    ") as dag:\n",
    "    \n",
    "    # Tasks\n",
    "    start = EmptyOperator(task_id='start')\n",
    "    \n",
    "    pull_data = PythonOperator(\n",
    "        task_id='dvc_pull_data',\n",
    "        python_callable=dvc_pull_data,\n",
    "    )\n",
    "    \n",
    "    validate = PythonOperator(\n",
    "        task_id='validate_data',\n",
    "        python_callable=validate_data,\n",
    "    )\n",
    "    \n",
    "    train = PythonOperator(\n",
    "        task_id='train_model',\n",
    "        python_callable=train_model,\n",
    "    )\n",
    "    \n",
    "    evaluate = BranchOperator(\n",
    "        task_id='evaluate_model',\n",
    "        python_callable=evaluate_model,\n",
    "    )\n",
    "    \n",
    "    register = PythonOperator(\n",
    "        task_id='register_model',\n",
    "        python_callable=register_model,\n",
    "    )\n",
    "    \n",
    "    skip = EmptyOperator(task_id='skip_registration')\n",
    "    \n",
    "    end = EmptyOperator(\n",
    "        task_id='end',\n",
    "        trigger_rule='none_failed_min_one_success'\n",
    "    )\n",
    "    \n",
    "    # Dependencies\n",
    "    start >> pull_data >> validate >> train >> evaluate\n",
    "    evaluate >> [register, skip]\n",
    "    [register, skip] >> end\n",
    "'''\n",
    "\n",
    "print(\"Complete ML Training Pipeline:\")\n",
    "print(\"(Copy this to ~/airflow/dags/ml_training_pipeline.py)\\n\")\n",
    "print(full_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Testing DAGs\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "```bash\n",
    "# Test if DAG has syntax errors\n",
    "python ~/airflow/dags/ml_training_pipeline.py\n",
    "\n",
    "# List DAGs\n",
    "airflow dags list | grep ml_training\n",
    "\n",
    "# Test specific task\n",
    "airflow tasks test ml_training_pipeline train_model 2024-01-01\n",
    "\n",
    "# Trigger DAG manually\n",
    "airflow dags trigger ml_training_pipeline\n",
    "\n",
    "# Check DAG runs\n",
    "airflow dags list-runs -d ml_training_pipeline\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Monitoring & Debugging\n",
    "\n",
    "### View Logs\n",
    "```bash\n",
    "# View task logs\n",
    "airflow tasks logs ml_training_pipeline train_model 2024-01-01\n",
    "\n",
    "# Or in UI: DAG ‚Üí Run ‚Üí Task ‚Üí Logs\n",
    "```\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Issue 1: Task fails intermittently**\n",
    "```python\n",
    "default_args = {\n",
    "    'retries': 3,  # Retry 3 times\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "```\n",
    "\n",
    "**Issue 2: Task stuck in running state**\n",
    "```bash\n",
    "# Clear task state\n",
    "airflow tasks clear ml_training_pipeline train_model\n",
    "```\n",
    "\n",
    "**Issue 3: DAG not showing up**\n",
    "```bash\n",
    "# Check import errors\n",
    "airflow dags list-import-errors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Production Best Practices\n",
    "\n",
    "### 1. Use Task Groups\n",
    "```python\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "\n",
    "with TaskGroup('training_group') as training:\n",
    "    train_lr = PythonOperator(...)\n",
    "    train_rf = PythonOperator(...)\n",
    "    train_xgb = PythonOperator(...)\n",
    "```\n",
    "\n",
    "### 2. Error Notifications\n",
    "```python\n",
    "default_args = {\n",
    "    'email': ['team@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Resource Management\n",
    "```python\n",
    "# Limit parallel tasks on t3.small (2GB RAM)\n",
    "# In airflow.cfg:\n",
    "# parallelism = 4\n",
    "# dag_concurrency = 4\n",
    "```\n",
    "\n",
    "### 4. Idempotency\n",
    "Tasks should be safe to re-run:\n",
    "```python\n",
    "def save_results():\n",
    "    # BAD: Append to file (duplicates on re-run)\n",
    "    # df.to_csv('results.csv', mode='a')\n",
    "    \n",
    "    # GOOD: Overwrite with timestamp\n",
    "    df.to_csv(f'results_{datetime.now():%Y%m%d}.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "### What We Learned:\n",
    "1. ‚úÖ **DAG Structure**: Python file defining workflow\n",
    "2. ‚úÖ **Task Types**: PythonOperator, BashOperator, BranchOperator\n",
    "3. ‚úÖ **Dependencies**: >>, parallel execution\n",
    "4. ‚úÖ **XCom**: Pass data between tasks\n",
    "5. ‚úÖ **Scheduling**: @daily, cron expressions\n",
    "6. ‚úÖ **Error Handling**: Retries, failure alerts\n",
    "\n",
    "### Real-World Benefits:\n",
    "- üöÄ **Automation**: No manual intervention\n",
    "- üîÑ **Reliability**: Automatic retries\n",
    "- üìä **Visibility**: See what's running/failed\n",
    "- ‚è∞ **Scheduling**: Run at optimal times\n",
    "- üë• **Team Coordination**: Everyone sees pipeline status\n",
    "\n",
    "### Before Airflow:\n",
    "\"Sarah, did you run the training script?\"\n",
    "\"Sarah, why did it fail?\"\n",
    "\"Sarah, can you run it again?\"\n",
    "\n",
    "### After Airflow:\n",
    "Pipeline runs automatically every day at 2 AM.\n",
    "If it fails, retries 3 times.\n",
    "Sends alert if still fails.\n",
    "Team checks dashboard for status.\n",
    "Sarah sleeps peacefully. üò¥\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** `07_Complete_Pipeline_Walkthrough.ipynb` - Put it all together!\n",
    "\n",
    "**¬© 2024 Amey Talkatkar**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
