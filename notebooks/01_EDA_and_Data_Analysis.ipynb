{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Notebook 1: Exploratory Data Analysis (EDA) and Data Understanding\n",
    "\n",
    "**Author:** Amey Talkatkar  \n",
    "**Email:** ameytalkatkar169@gmail.com  \n",
    "**GitHub:** https://github.com/ameytrainer  \n",
    "**Course:** MLOps with Agentic AI - Advanced Certification  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Load and understand the sales dataset structure\n",
    "- ‚úÖ Perform comprehensive exploratory data analysis\n",
    "- ‚úÖ Identify data quality issues\n",
    "- ‚úÖ Discover patterns and trends in sales data\n",
    "- ‚úÖ Understand feature relationships and correlations\n",
    "- ‚úÖ Make data-driven decisions for feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "## üî• The Problem (Why EDA Matters)\n",
    "\n",
    "**Real-World Disaster Story:**\n",
    "\n",
    "A data scientist was given a sales dataset and immediately jumped into training models:\n",
    "- Trained XGBoost without looking at the data\n",
    "- Got 95% accuracy! Celebrated! üéâ\n",
    "- Deployed to production\n",
    "\n",
    "**In production:**\n",
    "- Model predicted negative sales\n",
    "- Model predicted $10 million for a $50 item\n",
    "- Model failed on weekends (no weekend data in training!)\n",
    "\n",
    "**What went wrong?**\n",
    "- Never checked data distributions\n",
    "- Never found outliers\n",
    "- Never understood seasonal patterns\n",
    "- Never validated assumptions\n",
    "\n",
    "**This is why EDA is CRITICAL before any modeling.**\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üì¶ Pandas version: {pd.__version__}\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 1: Load Data\n",
    "\n",
    "First, let's generate and load our sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data if not exists\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "data_path = '../data/raw/sales_data.csv'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"üìä Generating sales data...\")\n",
    "    result = subprocess.run(\n",
    "        ['python', '../data/generate_data.py', '--rows', '10000', '--output', data_path],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(f\"‚úÖ Data already exists at: {data_path}\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path, parse_dates=['date'])\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2: Initial Data Inspection\n",
    "\n",
    "Let's understand the structure and content of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "print(\"üìã First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìã Last 5 rows:\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage\n",
    "print(\"üìä Data Types and Memory Usage:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\nüíæ Total Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"üîç Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Total missing values: {missing.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 3: Statistical Summary\n",
    "\n",
    "Understanding the distribution of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical statistics\n",
    "print(\"üìä Numerical Features Summary:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Additional percentiles\n",
    "print(\"\\nüìä Extended Percentiles:\")\n",
    "display(df[['price', 'quantity', 'sales']].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "print(\"üìä Categorical Features:\")\n",
    "categorical_cols = ['region', 'product', 'category', 'season']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    value_counts = df[col].value_counts()\n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"  {value:15s}: {count:6,} ({count/len(df)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Distribution Analysis\n",
    "\n",
    "Visualizing the distributions of key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Distribution of Numerical Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Price distribution\n",
    "axes[0, 0].hist(df['price'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Price Distribution')\n",
    "axes[0, 0].set_xlabel('Price ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df['price'].mean(), color='red', linestyle='--', label=f'Mean: ${df[\"price\"].mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Quantity distribution\n",
    "axes[0, 1].hist(df['quantity'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Quantity Distribution')\n",
    "axes[0, 1].set_xlabel('Quantity')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(df['quantity'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"quantity\"].mean():.1f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Sales distribution\n",
    "axes[0, 2].hist(df['sales'], bins=50, color='lightcoral', edgecolor='black')\n",
    "axes[0, 2].set_title('Sales Distribution')\n",
    "axes[0, 2].set_xlabel('Sales ($)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].axvline(df['sales'].mean(), color='red', linestyle='--', label=f'Mean: ${df[\"sales\"].mean():,.2f}')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Box plots for outlier detection\n",
    "df.boxplot(column='price', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Price Box Plot (Outlier Detection)')\n",
    "axes[1, 0].set_ylabel('Price ($)')\n",
    "\n",
    "df.boxplot(column='quantity', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Quantity Box Plot')\n",
    "axes[1, 1].set_ylabel('Quantity')\n",
    "\n",
    "df.boxplot(column='sales', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Sales Box Plot')\n",
    "axes[1, 2].set_ylabel('Sales ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Insights:\")\n",
    "print(f\"  - Price ranges from ${df['price'].min():.2f} to ${df['price'].max():.2f}\")\n",
    "print(f\"  - Quantity ranges from {df['quantity'].min()} to {df['quantity'].max()}\")\n",
    "print(f\"  - Sales range from ${df['sales'].min():.2f} to ${df['sales'].max():.2f}\")\n",
    "print(f\"  - Check for outliers in box plots above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÖ Step 5: Time Series Analysis\n",
    "\n",
    "Understanding temporal patterns is crucial for sales forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily sales trend\n",
    "daily_sales = df.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales['date'], daily_sales['sales'], linewidth=1, alpha=0.7)\n",
    "plt.title('Daily Sales Trend Over Time', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add rolling average\n",
    "daily_sales['rolling_7d'] = daily_sales['sales'].rolling(window=7).mean()\n",
    "daily_sales['rolling_30d'] = daily_sales['sales'].rolling(window=30).mean()\n",
    "plt.plot(daily_sales['date'], daily_sales['rolling_7d'], linewidth=2, label='7-day MA', color='red')\n",
    "plt.plot(daily_sales['date'], daily_sales['rolling_30d'], linewidth=2, label='30-day MA', color='green')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Insights:\")\n",
    "print(\"  - Look for trends (upward/downward)\")\n",
    "print(\"  - Identify seasonality patterns\")\n",
    "print(\"  - Check for anomalies or spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sales analysis\n",
    "monthly_sales = df.groupby('month')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "monthly_sales.columns = ['Month', 'Total Sales', 'Avg Sales', 'Num Transactions']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Monthly total sales\n",
    "axes[0].bar(monthly_sales['Month'], monthly_sales['Total Sales'], color='steelblue')\n",
    "axes[0].set_title('Total Sales by Month', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Total Sales ($)')\n",
    "axes[0].set_xticks(range(1, 13))\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Monthly average sales\n",
    "axes[1].bar(monthly_sales['Month'], monthly_sales['Avg Sales'], color='coral')\n",
    "axes[1].set_title('Average Sales by Month', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Average Sales ($)')\n",
    "axes[1].set_xticks(range(1, 13))\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Monthly Statistics:\")\n",
    "display(monthly_sales)\n",
    "\n",
    "# Find peak months\n",
    "peak_month = monthly_sales.loc[monthly_sales['Total Sales'].idxmax(), 'Month']\n",
    "print(f\"\\nüí° Peak sales month: {peak_month} (November/December = holiday season!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of week analysis\n",
    "dow_sales = df.groupby('day_of_week')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "dow_sales['day_name'] = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Sales by day of week\n",
    "axes[0].bar(dow_sales['day_name'], dow_sales['sum'], color='teal')\n",
    "axes[0].set_title('Total Sales by Day of Week', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Day of Week')\n",
    "axes[0].set_ylabel('Total Sales ($)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Weekend vs Weekday\n",
    "weekend_sales = df[df['is_weekend'] == True]['sales'].sum()\n",
    "weekday_sales = df[df['is_weekend'] == False]['sales'].sum()\n",
    "axes[1].bar(['Weekday', 'Weekend'], [weekday_sales, weekend_sales], color=['lightblue', 'orange'])\n",
    "axes[1].set_title('Weekday vs Weekend Sales', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Total Sales ($)')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Insights:\")\n",
    "print(f\"  - Weekend sales: ${weekend_sales:,.2f}\")\n",
    "print(f\"  - Weekday sales: ${weekday_sales:,.2f}\")\n",
    "print(f\"  - Weekend boost: {(weekend_sales/weekday_sales - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Step 6: Regional and Product Analysis\n",
    "\n",
    "Understanding regional and product-level patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by region\n",
    "region_sales = df.groupby('region')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "region_sales = region_sales.sort_values('sum', ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Total sales by region\n",
    "axes[0].barh(region_sales['region'], region_sales['sum'], color='skyblue')\n",
    "axes[0].set_title('Total Sales by Region', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Total Sales ($)')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(region_sales['sum'], labels=region_sales['region'], autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Sales Distribution by Region', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Regional Statistics:\")\n",
    "display(region_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by product\n",
    "product_sales = df.groupby('product')['sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "product_sales = product_sales.sort_values('sum', ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Total sales by product\n",
    "axes[0].barh(product_sales['product'], product_sales['sum'], color='lightgreen')\n",
    "axes[0].set_title('Total Sales by Product', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Total Sales ($)')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Average transaction size\n",
    "axes[1].barh(product_sales['product'], product_sales['mean'], color='coral')\n",
    "axes[1].set_title('Average Transaction Size by Product', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Avg Sales per Transaction ($)')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Product Statistics:\")\n",
    "display(product_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 7: Correlation Analysis\n",
    "\n",
    "Understanding relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "numeric_cols = ['price', 'quantity', 'sales', 'month', 'day_of_week']\n",
    "correlation = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Correlations:\")\n",
    "print(f\"  - Sales vs Price: {correlation.loc['sales', 'price']:.3f}\")\n",
    "print(f\"  - Sales vs Quantity: {correlation.loc['sales', 'quantity']:.3f}\")\n",
    "print(\"\\nüìù Note: Sales = Price √ó Quantity (perfect correlation expected!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for relationships\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Price vs Sales\n",
    "axes[0].scatter(df['price'], df['sales'], alpha=0.3, s=10)\n",
    "axes[0].set_title('Price vs Sales', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Price ($)')\n",
    "axes[0].set_ylabel('Sales ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Quantity vs Sales\n",
    "axes[1].scatter(df['quantity'], df['sales'], alpha=0.3, s=10, color='green')\n",
    "axes[1].set_title('Quantity vs Sales', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Quantity')\n",
    "axes[1].set_ylabel('Sales ($)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Advanced Analysis - Seasonality Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series for decomposition\n",
    "daily_sales_ts = df.groupby('date')['sales'].sum().reset_index()\n",
    "daily_sales_ts.set_index('date', inplace=True)\n",
    "\n",
    "# Simple moving averages to identify trend and seasonality\n",
    "daily_sales_ts['MA_7'] = daily_sales_ts['sales'].rolling(window=7).mean()\n",
    "daily_sales_ts['MA_30'] = daily_sales_ts['sales'].rolling(window=30).mean()\n",
    "daily_sales_ts['MA_90'] = daily_sales_ts['sales'].rolling(window=90).mean()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_sales_ts.index, daily_sales_ts['sales'], alpha=0.3, label='Daily Sales')\n",
    "plt.plot(daily_sales_ts.index, daily_sales_ts['MA_7'], linewidth=2, label='7-day MA')\n",
    "plt.plot(daily_sales_ts.index, daily_sales_ts['MA_30'], linewidth=2, label='30-day MA')\n",
    "plt.plot(daily_sales_ts.index, daily_sales_ts['MA_90'], linewidth=2, label='90-day MA (Trend)')\n",
    "plt.title('Sales Trend with Moving Averages', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Insights:\")\n",
    "print(\"  - 7-day MA: Captures weekly patterns\")\n",
    "print(\"  - 30-day MA: Smooths out weekly noise\")\n",
    "print(\"  - 90-day MA: Shows overall trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 9: Data Quality Report\n",
    "\n",
    "Summary of data quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset Size: {len(df):,} rows √ó {len(df.columns)} columns\")\n",
    "\n",
    "print(f\"\\n‚úÖ Missing Values: {df.isnull().sum().sum()} (0.00%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Duplicate Rows: {df.duplicated().sum()} (0.00%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Date Range:\")\n",
    "print(f\"   Start: {df['date'].min().date()}\")\n",
    "print(f\"   End:   {df['date'].max().date()}\")\n",
    "print(f\"   Days:  {(df['date'].max() - df['date'].min()).days}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Numerical Ranges:\")\n",
    "print(f\"   Price:    ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "print(f\"   Quantity: {df['quantity'].min()} - {df['quantity'].max()}\")\n",
    "print(f\"   Sales:    ${df['sales'].min():.2f} - ${df['sales'].max():.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Categorical Values:\")\n",
    "print(f\"   Regions:  {df['region'].nunique()} unique ({', '.join(df['region'].unique())})\")\n",
    "print(f\"   Products: {df['product'].nunique()} unique ({', '.join(df['product'].unique())})\")\n",
    "print(f\"   Seasons:  {df['season'].nunique()} unique ({', '.join(df['season'].unique())})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Business Logic Checks:\")\n",
    "calculated_sales = df['price'] * df['quantity']\n",
    "sales_match = (calculated_sales - df['sales']).abs().max() < 0.01\n",
    "print(f\"   Sales = Price √ó Quantity: {'PASS ‚úÖ' if sales_match else 'FAIL ‚ùå'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY FOR FEATURE ENGINEERING!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. ‚úÖ **Data Structure**: 10,000+ rows with date, region, product, price, quantity, sales\n",
    "2. ‚úÖ **No Quality Issues**: No missing values, no duplicates, data ranges are reasonable\n",
    "3. ‚úÖ **Seasonality**: Strong Q4 boost (November-December holiday season)\n",
    "4. ‚úÖ **Day-of-Week Effect**: Weekend sales are higher than weekdays\n",
    "5. ‚úÖ **Regional Patterns**: Different regions have different product preferences\n",
    "6. ‚úÖ **Growth Trend**: Overall upward trend in sales over time\n",
    "\n",
    "### Why This Matters for MLOps:\n",
    "- üìä **Feature Engineering**: We now know which patterns to capture (seasonality, day-of-week)\n",
    "- üîÑ **Data Validation**: We'll create checks based on these ranges (price $10-$1000, quantity 1-100)\n",
    "- üìà **Model Selection**: Time series patterns suggest we need features for trends and seasonality\n",
    "- üéØ **Monitoring**: We know what \"normal\" looks like (for drift detection later)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Next Notebook:** `02_Feature_Engineering.ipynb`\n",
    "\n",
    "We'll use these insights to:\n",
    "- Create lag features (yesterday's sales, last week's sales)\n",
    "- Encode categorical variables (region, product)\n",
    "- Create seasonal indicators\n",
    "- Scale numerical features\n",
    "- Split into train/test sets\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Amey Talkatkar | **Course:** MLOps with Agentic AI\n",
    "\n",
    "**¬© 2024 Amey Talkatkar** | Educational Use License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
