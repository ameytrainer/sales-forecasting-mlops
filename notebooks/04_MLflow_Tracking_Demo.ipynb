{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Notebook 4: MLflow Experiment Tracking\n",
    "\n",
    "**Author:** Amey Talkatkar | **Course:** MLOps with Agentic AI\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Connect to MLflow tracking server (DagsHub)\n",
    "- Log parameters, metrics, and artifacts\n",
    "- Compare multiple experiment runs\n",
    "- Use MLflow Model Registry\n",
    "- Load models from registry\n",
    "- Transition models through stages\n",
    "\n",
    "## ðŸ”¥ The Problem (Solved!)\n",
    "From Notebook 3, we had:\n",
    "- 3 models trained\n",
    "- No systematic tracking\n",
    "- Can't reproduce experiments\n",
    "- Can't compare hyperparameters easily\n",
    "\n",
    "**MLflow solves all of this!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure MLflow\n",
    "\n",
    "Set up connection to MLflow tracking server (DagsHub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: DagsHub (Recommended)\n",
    "# Get these from your .env file\n",
    "MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', 'https://dagshub.com/YOUR_USERNAME/sales-forecasting-mlops.mlflow')\n",
    "MLFLOW_TRACKING_USERNAME = os.getenv('MLFLOW_TRACKING_USERNAME', 'YOUR_USERNAME')\n",
    "MLFLOW_TRACKING_PASSWORD = os.getenv('MLFLOW_TRACKING_PASSWORD', 'YOUR_TOKEN')\n",
    "\n",
    "# Set credentials\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = MLFLOW_TRACKING_USERNAME\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = MLFLOW_TRACKING_PASSWORD\n",
    "\n",
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "print(f\"âœ… MLflow configured\")\n",
    "print(f\"   Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    experiments = mlflow.search_experiments()\n",
    "    print(f\"   âœ… Connection successful! Found {len(experiments)} experiments\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Connection failed: {e}\")\n",
    "    print(\"   ðŸ’¡ Check your credentials in .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment (creates if doesn't exist)\n",
    "EXPERIMENT_NAME = \"sales_forecasting_notebook\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "print(f\"âœ… Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"   ID: {experiment.experiment_id}\")\n",
    "print(f\"   Artifact Location: {experiment.artifact_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv').squeeze()\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').squeeze()\n",
    "\n",
    "print(f\"âœ… Data loaded: {len(X_train):,} train, {len(X_test):,} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train with MLflow Tracking\n",
    "\n",
    "### Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"linear_regression_v1\") as run:\n",
    "    print(f\"ðŸ”¹ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"fit_intercept\", True)\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Log dataset info\n",
    "    mlflow.log_param(\"train_samples\", len(X_train))\n",
    "    mlflow.log_param(\"test_samples\", len(X_test))\n",
    "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "    \n",
    "    print(f\"âœ… Logged: RMSE=${rmse:,.2f}, RÂ²={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"random_forest_v1\") as run:\n",
    "    print(f\"ðŸŒ² Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Model parameters\n",
    "    params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_samples_split\": 20,\n",
    "        \"min_samples_leaf\": 10,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Log all parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Train\n",
    "    model = RandomForestRegressor(**params, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2_score\": r2\n",
    "    })\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(f\"âœ… Logged: RMSE=${rmse:,.2f}, RÂ²={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"xgboost_v1\") as run:\n",
    "    print(f\"âš¡ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Model parameters\n",
    "    params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_child_weight\": 3,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"XGBoost\")\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Train\n",
    "    model = XGBRegressor(**params, n_jobs=-1, verbosity=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2_score\": r2\n",
    "    })\n",
    "    \n",
    "    # Log model (XGBoost native format)\n",
    "    mlflow.xgboost.log_model(model, \"model\")\n",
    "    \n",
    "    # Save run ID for later\n",
    "    best_run_id = run.info.run_id\n",
    "    \n",
    "    print(f\"âœ… Logged: RMSE=${rmse:,.2f}, RÂ²={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search all runs in experiment\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.rmse ASC\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š All Runs (sorted by RMSE):\")\n",
    "display(runs[[\n",
    "    'run_id', \n",
    "    'tags.mlflow.runName',\n",
    "    'params.model_type',\n",
    "    'metrics.rmse',\n",
    "    'metrics.mae',\n",
    "    'metrics.r2_score'\n",
    "]].head())\n",
    "\n",
    "best_run = runs.iloc[0]\n",
    "print(f\"\\nðŸ† Best Model: {best_run['params.model_type']}\")\n",
    "print(f\"   Run ID: {best_run['run_id']}\")\n",
    "print(f\"   RMSE: ${best_run['metrics.rmse']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Register Best Model\n",
    "\n",
    "Move model to MLflow Model Registry for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model\n",
    "MODEL_NAME = \"sales_forecasting_model\"\n",
    "model_uri = f\"runs:/{best_run['run_id']}/model\"\n",
    "\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model registered!\")\n",
    "print(f\"   Name: {model_version.name}\")\n",
    "print(f\"   Version: {model_version.version}\")\n",
    "print(f\"   Stage: {model_version.current_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Transition Model to Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLflow client\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# Transition to Staging\n",
    "client.transition_model_version_stage(\n",
    "    name=MODEL_NAME,\n",
    "    version=model_version.version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model transitioned to Staging\")\n",
    "print(f\"   Ready for validation tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Load Model from Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from registry (Staging stage)\n",
    "model_uri = f\"models:/{MODEL_NAME}/Staging\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "print(f\"âœ… Model loaded from registry\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Stage: Staging\")\n",
    "\n",
    "# Make test prediction\n",
    "sample_prediction = loaded_model.predict(X_test.head(5))\n",
    "print(f\"\\nðŸ“Š Sample predictions:\")\n",
    "for i, pred in enumerate(sample_prediction):\n",
    "    actual = y_test.iloc[i]\n",
    "    print(f\"   {i+1}. Predicted: ${pred:,.2f}, Actual: ${actual:,.2f}, Error: ${abs(pred-actual):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: View on DagsHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŒ View your experiments on DagsHub:\")\n",
    "print(f\"   {MLFLOW_TRACKING_URI.replace('.mlflow', '')}\")\n",
    "print(\"\\n   Navigate to: Experiments tab > {EXPERIMENT_NAME}\")\n",
    "print(\"   You'll see all 3 runs with metrics and parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "1. âœ… Connected to MLflow (DagsHub)\n",
    "2. âœ… Logged 3 model experiments with parameters and metrics\n",
    "3. âœ… Compared runs systematically\n",
    "4. âœ… Registered best model to Model Registry\n",
    "5. âœ… Transitioned model to Staging\n",
    "6. âœ… Loaded model from registry\n",
    "\n",
    "### Why This Matters for MLOps:\n",
    "- ðŸ”„ **Reproducibility**: Every experiment tracked\n",
    "- ðŸ“Š **Comparison**: Easy to see what works\n",
    "- ðŸŽ¯ **Model Registry**: Central repository for production models\n",
    "- ðŸš€ **Deployment Ready**: Load models by stage (Staging/Production)\n",
    "- ðŸ‘¥ **Collaboration**: Team can see all experiments\n",
    "\n",
    "### Model Lifecycle:\n",
    "```\n",
    "Training â†’ Register â†’ Staging â†’ Validate â†’ Production â†’ Archive\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** `05_DVC_Data_Versioning.ipynb` - Version control for data\n",
    "\n",
    "**Â© 2024 Amey Talkatkar**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
